{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data1 = pd.read_csv(os.getcwd() + '/Dataset/Data1_Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data1.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove none alphabetic characters\n",
    "import re\n",
    "data = re.sub('[^A-Za-z]', ' ', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the word lower case\n",
    "data = data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/bongsooyi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_data = word_tokenize(data)\n",
    "# print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Remove the stop words\\nnltk.download('stopwords')\\nfrom nltk.corpus import stopwords \\n\\nstopwords.words('english')\\nfor word in tokenized_data:\\n    if word in stopwords.words('english'):\\n        tokenized_sms.remove(word)\\n\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Remove the stop words\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "stopwords.words('english')\n",
    "for word in tokenized_data:\n",
    "    if word in stopwords.words('english'):\n",
    "        tokenized_sms.remove(word)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/bongsooyi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "'''\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for i in range(len(tokenized_data)):\n",
    "    tokenized_data[i] = stemmer.stem(tokenized_data[i])\n",
    "    \n",
    "print(tokenized_data)\n",
    "'''\n",
    "\n",
    "nltk.download('wordnet') \n",
    "\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "for i in range(len(tokenized_data)):\n",
    "    tokenized_data[i] = lemma.lemmatize(tokenized_data[i])\n",
    "    \n",
    "#print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell correction\n",
    "# !pip install autocorrect\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "for i in range(len(tokenized_data)):\n",
    "    tokenized_data[i] = spell(tokenized_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = \" \".join(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the feature matrix \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([data_text]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about',\n",
       " 'above',\n",
       " 'accept',\n",
       " 'alan',\n",
       " 'alansandoval',\n",
       " 'albert',\n",
       " 'all',\n",
       " 'allow',\n",
       " 'also',\n",
       " 'always',\n",
       " 'america',\n",
       " 'american',\n",
       " 'and',\n",
       " 'andrew',\n",
       " 'angry',\n",
       " 'anniversary',\n",
       " 'apart',\n",
       " 'are',\n",
       " 'at',\n",
       " 'badly',\n",
       " 'be',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'bishop',\n",
       " 'breaking',\n",
       " 'burton',\n",
       " 'but',\n",
       " 'by',\n",
       " 'calvin',\n",
       " 'calvinstowell',\n",
       " 'can',\n",
       " 'change',\n",
       " 'citizen',\n",
       " 'clearly',\n",
       " 'com',\n",
       " 'congress',\n",
       " 'control',\n",
       " 'couldn',\n",
       " 'country',\n",
       " 'dale',\n",
       " 'daniel',\n",
       " 'december',\n",
       " 'decency',\n",
       " 'despicable',\n",
       " 'did',\n",
       " 'difference',\n",
       " 'directed',\n",
       " 'dishonest',\n",
       " 'do',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'donald',\n",
       " 'down',\n",
       " 'easter',\n",
       " 'enemy',\n",
       " 'enough',\n",
       " 'eve',\n",
       " 'even',\n",
       " 'expect',\n",
       " 'fae',\n",
       " 'fake',\n",
       " 'few',\n",
       " 'filter',\n",
       " 'for',\n",
       " 'former',\n",
       " 'fought',\n",
       " 'friend',\n",
       " 'from',\n",
       " 'fucking',\n",
       " 'gibberish',\n",
       " 'give',\n",
       " 'goodbye',\n",
       " 'got',\n",
       " 'great',\n",
       " 'greeting',\n",
       " 'grows',\n",
       " 'gutter',\n",
       " 'ha',\n",
       " 'had',\n",
       " 'hallmark',\n",
       " 'happy',\n",
       " 'hate',\n",
       " 'have',\n",
       " 'he',\n",
       " 'healthy',\n",
       " 'hear',\n",
       " 'here',\n",
       " 'him',\n",
       " 'his',\n",
       " 'hole',\n",
       " 'holiday',\n",
       " 'how',\n",
       " 'image',\n",
       " 'impeachment',\n",
       " 'in',\n",
       " 'include',\n",
       " 'including',\n",
       " 'infantile',\n",
       " 'instead',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'job',\n",
       " 'just',\n",
       " 'kind',\n",
       " 'know',\n",
       " 'korea',\n",
       " 'korencarpenter',\n",
       " 'kya',\n",
       " 'lack',\n",
       " 'last',\n",
       " 'later',\n",
       " 'leave',\n",
       " 'like',\n",
       " 'll',\n",
       " 'long',\n",
       " 'lost',\n",
       " 'love',\n",
       " 'make',\n",
       " 'many',\n",
       " 'marine',\n",
       " 'me',\n",
       " 'medium',\n",
       " 'men',\n",
       " 'message',\n",
       " 'miranda',\n",
       " 'mirandayaver',\n",
       " 'my',\n",
       " 'new',\n",
       " 'news',\n",
       " 'no',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'of',\n",
       " 'old',\n",
       " 'older',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'our',\n",
       " 'out',\n",
       " 'part',\n",
       " 'people',\n",
       " 'petty',\n",
       " 'photo',\n",
       " 'pic',\n",
       " 'polling',\n",
       " 'power',\n",
       " 'president',\n",
       " 'presidential',\n",
       " 'rainyday',\n",
       " 'rapidly',\n",
       " 'realdonaldtrump',\n",
       " 'reality',\n",
       " 'regaining',\n",
       " 'rise',\n",
       " 'roy',\n",
       " 'sandoval',\n",
       " 'say',\n",
       " 'schalke',\n",
       " 'sends',\n",
       " 'sgoodine',\n",
       " 'shout',\n",
       " 'show',\n",
       " 'smarter',\n",
       " 'so',\n",
       " 'star',\n",
       " 'steven',\n",
       " 'stronger',\n",
       " 'supporter',\n",
       " 'swan',\n",
       " 'talbertswan',\n",
       " 'talk',\n",
       " 'teenager',\n",
       " 'term',\n",
       " 'thanksgiving',\n",
       " 'that',\n",
       " 'thbthttt',\n",
       " 'the',\n",
       " 'they',\n",
       " 'thinking',\n",
       " 'this',\n",
       " 'those',\n",
       " 'to',\n",
       " 'trump',\n",
       " 'tweet',\n",
       " 'tweeted',\n",
       " 'twitter',\n",
       " 'us',\n",
       " 'very',\n",
       " 'voted',\n",
       " 'want',\n",
       " 'well',\n",
       " 'wendy',\n",
       " 'wendywhistles',\n",
       " 'went',\n",
       " 'were',\n",
       " 'what',\n",
       " 'when',\n",
       " 'who',\n",
       " 'why',\n",
       " 'will',\n",
       " 'wish',\n",
       " 'won',\n",
       " 'wonder',\n",
       " 'word',\n",
       " 'work',\n",
       " 'would',\n",
       " 'wrong',\n",
       " 'year',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yourself']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'donald': 52,\n",
       " 'trump': 186,\n",
       " 'just': 104,\n",
       " 'couldn': 38,\n",
       " 'wish': 204,\n",
       " 'all': 6,\n",
       " 'american': 11,\n",
       " 'happy': 81,\n",
       " 'new': 129,\n",
       " 'year': 211,\n",
       " 'and': 12,\n",
       " 'leave': 113,\n",
       " 'it': 102,\n",
       " 'at': 18,\n",
       " 'that': 178,\n",
       " 'instead': 99,\n",
       " 'he': 84,\n",
       " 'had': 79,\n",
       " 'to': 185,\n",
       " 'give': 71,\n",
       " 'shout': 164,\n",
       " 'out': 142,\n",
       " 'his': 89,\n",
       " 'enemy': 55,\n",
       " 'later': 112,\n",
       " 'the': 180,\n",
       " 'very': 191,\n",
       " 'dishonest': 48,\n",
       " 'fake': 61,\n",
       " 'news': 130,\n",
       " 'medium': 123,\n",
       " 'former': 65,\n",
       " 'reality': 155,\n",
       " 'show': 165,\n",
       " 'star': 168,\n",
       " 'one': 139,\n",
       " 'job': 103,\n",
       " 'do': 49,\n",
       " 'our': 141,\n",
       " 'country': 39,\n",
       " 'rapidly': 153,\n",
       " 'grows': 76,\n",
       " 'stronger': 170,\n",
       " 'smarter': 166,\n",
       " 'want': 193,\n",
       " 'of': 135,\n",
       " 'my': 128,\n",
       " 'friend': 67,\n",
       " 'supporter': 171,\n",
       " 'even': 58,\n",
       " 'healthy': 85,\n",
       " 'president': 150,\n",
       " 'angry': 14,\n",
       " 'part': 143,\n",
       " 'tweeted': 188,\n",
       " 'will': 203,\n",
       " 'be': 20,\n",
       " 'great': 74,\n",
       " 'for': 64,\n",
       " 'america': 10,\n",
       " 'realdonaldtrump': 154,\n",
       " 'december': 42,\n",
       " 'tweet': 187,\n",
       " 'went': 197,\n",
       " 'down': 53,\n",
       " 'about': 0,\n",
       " 'well': 194,\n",
       " 'you': 212,\n",
       " 'expect': 59,\n",
       " 'what': 199,\n",
       " 'kind': 105,\n",
       " 'sends': 162,\n",
       " 'greeting': 75,\n",
       " 'like': 114,\n",
       " 'this': 183,\n",
       " 'despicable': 44,\n",
       " 'petty': 145,\n",
       " 'infantile': 98,\n",
       " 'gibberish': 70,\n",
       " 'only': 140,\n",
       " 'lack': 110,\n",
       " 'decency': 43,\n",
       " 'won': 205,\n",
       " 'allow': 7,\n",
       " 'him': 88,\n",
       " 'rise': 157,\n",
       " 'above': 1,\n",
       " 'gutter': 77,\n",
       " 'long': 116,\n",
       " 'enough': 56,\n",
       " 'citizen': 33,\n",
       " 'bishop': 24,\n",
       " 'albert': 5,\n",
       " 'swan': 172,\n",
       " 'talbertswan': 173,\n",
       " 'no': 131,\n",
       " 'calvin': 29,\n",
       " 'calvinstowell': 30,\n",
       " 'your': 213,\n",
       " 'impeachment': 94,\n",
       " 'would': 209,\n",
       " 'make': 119,\n",
       " 'but': 27,\n",
       " 'll': 115,\n",
       " 'also': 8,\n",
       " 'accept': 2,\n",
       " 'regaining': 156,\n",
       " 'control': 37,\n",
       " 'congress': 36,\n",
       " 'miranda': 126,\n",
       " 'mirandayaver': 127,\n",
       " 'hear': 86,\n",
       " 'yourself': 214,\n",
       " 'talk': 174,\n",
       " 'when': 200,\n",
       " 'have': 83,\n",
       " 'include': 96,\n",
       " 'many': 120,\n",
       " 'people': 144,\n",
       " 'hate': 82,\n",
       " 'wonder': 206,\n",
       " 'why': 202,\n",
       " 'they': 181,\n",
       " 'me': 122,\n",
       " 'alan': 3,\n",
       " 'sandoval': 159,\n",
       " 'alansandoval': 4,\n",
       " 'who': 201,\n",
       " 'us': 190,\n",
       " 'word': 207,\n",
       " 'in': 95,\n",
       " 'marine': 121,\n",
       " 'can': 31,\n",
       " 'say': 160,\n",
       " 'korea': 107,\n",
       " 'polling': 148,\n",
       " 'korencarpenter': 108,\n",
       " 'here': 87,\n",
       " 'eve': 57,\n",
       " 'from': 68,\n",
       " 'including': 97,\n",
       " 'those': 184,\n",
       " 'fought': 66,\n",
       " 'lost': 117,\n",
       " 'so': 167,\n",
       " 'badly': 19,\n",
       " 'don': 51,\n",
       " 'know': 106,\n",
       " 'love': 118,\n",
       " 'is': 101,\n",
       " 'nothing': 133,\n",
       " 'been': 22,\n",
       " 'doing': 50,\n",
       " 'ha': 78,\n",
       " 'directed': 47,\n",
       " 'message': 125,\n",
       " 'easter': 54,\n",
       " 'thanksgiving': 177,\n",
       " 'anniversary': 15,\n",
       " 'pic': 147,\n",
       " 'twitter': 189,\n",
       " 'com': 35,\n",
       " 'fae': 60,\n",
       " 'kya': 109,\n",
       " 'daniel': 41,\n",
       " 'dale': 40,\n",
       " 'holiday': 91,\n",
       " 'are': 17,\n",
       " 'clearly': 34,\n",
       " 'not': 132,\n",
       " 'presidential': 151,\n",
       " 'how': 92,\n",
       " 'did': 45,\n",
       " 'work': 208,\n",
       " 'hallmark': 80,\n",
       " 'before': 23,\n",
       " 'becoming': 21,\n",
       " 'steven': 169,\n",
       " 'goodbye': 72,\n",
       " 'sgoodine': 163,\n",
       " 'always': 9,\n",
       " 'difference': 46,\n",
       " 'last': 111,\n",
       " 'few': 62,\n",
       " 'filter': 63,\n",
       " 'breaking': 25,\n",
       " 'roy': 158,\n",
       " 'schalke': 161,\n",
       " 'thbthttt': 179,\n",
       " 'apart': 16,\n",
       " 'teenager': 175,\n",
       " 'term': 176,\n",
       " 'wendy': 195,\n",
       " 'wendywhistles': 196,\n",
       " 'fucking': 69,\n",
       " 'old': 136,\n",
       " 'rainyday': 152,\n",
       " 'voted': 192,\n",
       " 'hole': 90,\n",
       " 'thinking': 182,\n",
       " 'change': 32,\n",
       " 'once': 138,\n",
       " 'got': 73,\n",
       " 'into': 100,\n",
       " 'power': 149,\n",
       " 'were': 198,\n",
       " 'wrong': 210,\n",
       " 'men': 124,\n",
       " 'now': 134,\n",
       " 'older': 137,\n",
       " 'photo': 146,\n",
       " 'by': 28,\n",
       " 'andrew': 13,\n",
       " 'burton': 26,\n",
       " 'image': 93}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  1  1  1  1  6  1  1  1  3  2 14  1  1  1  1  1  2  1  2  1  3  1\n",
      "   1  1  1  1  1  1  1  1  2  1  1  1  1  1  2  2  2  1 13  1  1  1  1  1\n",
      "   3  5  1  2  3  2  1  5  1  1  3  1  1  3  1  1  7  1  1  2  2  1  1  1\n",
      "   1  1  3  1  2  1  2  2  1  6  2  3  9  2  1  1  1  4  1  1  1  1  1  2\n",
      "   1  1  1  1  1  2  2  1  4  1  2  1  1  1  1  1  6  1  3  1  2  1  1  1\n",
      "   2  2  2  3  1  1  1  1  3 11  3  1  1  1  1  6  2  1  1  2  2  2  1  1\n",
      "   2  2  1  1  1  1  3  1  1  2  2  1  1  1  1  1  1  1  1  1  1  1  2  2\n",
      "   1  1  2  2  1  1  1  1  1  1  4  1 13  2  1  5  1 14  9  3  1  1  2  3\n",
      "   1  2  1  1  1  1  1  2  1  5  1  2  5  1  1  1  1  2  1 19  8  1  1]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
