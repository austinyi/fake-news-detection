{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shengjie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Shengjie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_path = \"./data/processed_tsv/K1.tsv\"\n",
    "d2_path = \"./data/processed_tsv/K2.tsv\"\n",
    "d3_path = \"./data/processed_tsv/K3.tsv\"\n",
    "d4_path = \"./data/processed_tsv/LIAR.tsv\"\n",
    "\n",
    "d1 = pd.read_csv(d1_path, delimiter=\"\\t\")\n",
    "d2 = pd.read_csv(d2_path, delimiter=\"\\t\")\n",
    "d3 = pd.read_csv(d3_path, delimiter=\"\\t\")\n",
    "d4 = pd.read_csv(d4_path, delimiter=\"\\t\")\n",
    "\n",
    "d1['content'] = d1['content'].apply(lambda x: str(x))\n",
    "d2['content'] = d2['content'].apply(lambda x: str(x))\n",
    "d3['content'] = d3['content'].apply(lambda x: str(x))\n",
    "d4['content'] = d4['content'].apply(lambda x: str(x))\n",
    "\n",
    "d1['domain'] = 0\n",
    "d2['domain'] = 1\n",
    "d3['domain'] = 2\n",
    "d4['domain'] = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== d1 =====\n",
      "        label\n",
      "label       \n",
      "0      23481\n",
      "1      21417\n",
      "===== d2 =====\n",
      "        label\n",
      "label       \n",
      "0       4488\n",
      "1       5752\n",
      "===== d3 =====\n",
      "        label\n",
      "label       \n",
      "0       2137\n",
      "1       1872\n",
      "===== d4 =====\n",
      "        label\n",
      "label       \n",
      "0       3554\n",
      "1       4507\n"
     ]
    }
   ],
   "source": [
    "print('===== d1 =====\\n', d1.groupby(['label'])[['label']].count())\n",
    "print('===== d2 =====\\n', d2.groupby(['label'])[['label']].count())\n",
    "print('===== d3 =====\\n', d3.groupby(['label'])[['label']].count())\n",
    "print('===== d4 =====\\n', d4.groupby(['label'])[['label']].count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        self.porter = PorterStemmer()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "        text = str(text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"i'm\", \"i am\", text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"she's\", \"she is\", text)\n",
    "        text = re.sub(r\"it's\", \"it is\", text)\n",
    "        text = re.sub(r\"that's\", \"that is\", text)\n",
    "        text = re.sub(r\"what's\", \"that is\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text)\n",
    "        text = re.sub(r\"how's\", \"how is\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"can't\", \"cannot\", text)\n",
    "        text = re.sub(r\"n't\", \" not\", text)\n",
    "        text = re.sub(r\"n'\", \"ng\", text)\n",
    "        text = re.sub(r\"'bout\", \"about\", text)\n",
    "        text = re.sub(r\"'til\", \"until\", text)\n",
    "        text = re.sub(r\"[()\\\"_#/@;*%:{}<>`+=~|.!?,'$-\\[\\]]\", \"\", text)\n",
    "        text = re.sub(r\"[0-9]\", \"\", text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def lemmatizer(self, text):\n",
    "        sentence_words = nltk.word_tokenize(text)\n",
    "        ret_text = []\n",
    "        for word in sentence_words:\n",
    "            ret_text.append(self.wordnet_lemmatizer.lemmatize(word))\n",
    "\n",
    "        return \" \".join(ret_text)\n",
    "\n",
    "    def stemmer(self, text):\n",
    "        sentence_words = nltk.word_tokenize(text)\n",
    "        ret_text = []\n",
    "        for word in sentence_words:\n",
    "            ret_text.append(self.porter.stem(word))\n",
    "\n",
    "        return \" \".join(ret_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_data(df, lemmatize=False, stem=False):\n",
    "    preprocessor = Preprocess()\n",
    "    encoder_inputs = df['content'].apply(lambda x: preprocessor.clean_text(x))\n",
    "    if lemmatize:\n",
    "        encoder_inputs = df['content'].apply(\n",
    "            lambda x: preprocessor.lemmatizer(x))\n",
    "    if stem:\n",
    "        encoder_inputs = df['content'].apply(lambda x: preprocessor.stemmer(x))\n",
    "    encoder_inputs = np.array(encoder_inputs.values.tolist())\n",
    "    return encoder_inputs\n",
    "\n",
    "\n",
    "def fake_news_target(df):\n",
    "    return np.array(df['label'].apply(lambda x: int(x)).values.tolist())\n",
    "\n",
    "\n",
    "def domain_target(df):\n",
    "    return np.array(df['domain'].apply(lambda x: int(x)).values.tolist())\n",
    "\n",
    "\n",
    "class Shuffle:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.seed = np.random.randint(100000)\n",
    "\n",
    "    def shuffle(self, data):\n",
    "        np.random.seed(self.seed)\n",
    "        np.random.shuffle(data)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the word2idx, idx2word mapping using the Keras Tokenizer\n",
    "def create_vocab(text_lists, vocab_len):\n",
    "    tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
    "    tokenizer.fit_on_texts(text_lists)\n",
    "    # Due to ambiguity with regards to Keras Tokenizer num_words, below is a good enough fix,\n",
    "    # though it changes the tokenizer word_index outside of the class\n",
    "    num_words = vocab_len\n",
    "\n",
    "    sorted_by_word_count = sorted(\n",
    "        tokenizer.word_counts.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    tokenizer.word_index = {}\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    i = 0\n",
    "    for word, count in sorted_by_word_count:\n",
    "        if i == num_words:\n",
    "            break\n",
    "\n",
    "        # <= because tokenizer is 1 indexed\n",
    "        tokenizer.word_index[word] = i + 1\n",
    "        word2idx[word] = i+1\n",
    "        idx2word[i+1] = word\n",
    "        i += 1\n",
    "\n",
    "    tokenizer.word_index[tokenizer.oov_token] = num_words+1\n",
    "    word2idx[tokenizer.oov_token] = num_words+1\n",
    "    idx2word[num_words+1] = tokenizer.oov_token\n",
    "\n",
    "    return word2idx, idx2word, tokenizer\n",
    "\n",
    "\n",
    "def pad_tokenize_data(encoder_inputs, max_sentence_length, tokenizer):\n",
    "\n",
    "    t_encoder_inputs = tokenizer.texts_to_sequences(encoder_inputs)\n",
    "    t_encoder_inputs = pad_sequences(\n",
    "        t_encoder_inputs, maxlen=max_sentence_length, padding='post', truncating='post')\n",
    "\n",
    "    return t_encoder_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, Y_CC, Y_DC, max_sentence_length, word2idx, batch_size=128):\n",
    "\n",
    "    y_cc = to_categorical(Y_CC)\n",
    "    y_dc = to_categorical(Y_DC)\n",
    "    for idx in range(0, len(X), batch_size):\n",
    "        encoder_input = np.zeros((batch_size, max_sentence_length))\n",
    "        decoder_target = np.zeros(\n",
    "            (batch_size, max_sentence_length, len(word2idx)+1))\n",
    "        for j, input_seq in enumerate(X[idx:idx+batch_size]):\n",
    "            for i, word_idx in enumerate(input_seq):\n",
    "                encoder_input[j, i] = word_idx\n",
    "                decoder_target[j, i, word_idx] = 1\n",
    "\n",
    "        yield [encoder_input, [decoder_target, y_cc[idx:idx+batch_size], y_dc[idx:idx+batch_size]]]\n",
    "\n",
    "\n",
    "def all_data_generator(X, Y_CC, Y_DC, max_sentence_length):\n",
    "    encoder_input = np.zeros((len(X), max_sentence_length))\n",
    "    for j, input_seq in enumerate(X):\n",
    "        for i, word_idx in enumerate(input_seq):\n",
    "            encoder_input[j, i] = word_idx\n",
    "\n",
    "    y_cc = to_categorical(Y_CC)\n",
    "    y_dc = to_categorical(Y_DC)\n",
    "\n",
    "    return [encoder_input, y_cc, y_dc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for a single domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model(max_encoder_len, embedding_dim, latent_dim, vocab_len):\n",
    "    inputs = Input(shape=(max_encoder_len,), name=\"encoder_inputs\")\n",
    "\n",
    "    layer_embedding = Embedding(vocab_len+1, embedding_dim, trainable=True,\n",
    "                                input_length=max_encoder_len, mask_zero=True, name=\"encoder_embedding\")\n",
    "    layer_lstm = LSTM(latent_dim, return_state=True)\n",
    "    layer_nonlinear = Dense(128, activation=\"tanh\", name=\"non_linear\")\n",
    "    layer_softmax = Dense(2, activation=\"softmax\", name=\"softmax\")\n",
    "\n",
    "    embedded = layer_embedding(inputs)\n",
    "    encoded, _, _ = layer_lstm(embedded)\n",
    "    logits = layer_nonlinear(encoded)\n",
    "    outputs = layer_softmax(logits)\n",
    "\n",
    "    return Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain independent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_independent_model(max_encoder_len, embedding_dim, latent_dim, vocab_len, domain_count):\n",
    "    inputs = Input(shape=(max_encoder_len,), name=\"encoder_inputs\")\n",
    "\n",
    "    layer_embedding = Embedding(vocab_len+1, embedding_dim, trainable=True,\n",
    "                                input_length=max_encoder_len, mask_zero=True, name=\"encoder_embedding\")\n",
    "    layer_lstm = LSTM(latent_dim, return_state=True)\n",
    "    layer_nonlinear_cc = Dense(128, activation=\"tanh\", name=\"nonlinear_cc\")\n",
    "    layer_softmax_cc = Dense(2, activation=\"softmax\", name=\"softmax_cc\")\n",
    "    layer_nonlinear_dc = Dense(128, activation=\"tanh\", name=\"nonlinear_dc\")\n",
    "    layer_softmax_dc = Dense(domain_count, activation=\"softmax\", name=\"softmax_dc\")\n",
    "\n",
    "    embedded = layer_embedding(inputs)\n",
    "    encoded, _, _ = layer_lstm(embedded)\n",
    "    logits_cc = layer_nonlinear_cc(encoded)\n",
    "    outputs_cc = layer_softmax_cc(logits_cc)\n",
    "    logits_dc = layer_nonlinear_dc(encoded)\n",
    "    outputs_dc = layer_softmax_dc(logits_dc)\n",
    "\n",
    "    return Model(inputs, [outputs_cc, outputs_dc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(df, vocab_size, max_sentence_length):\n",
    "    encoder_inputs = encoder_data(df)\n",
    "    y_cc = fake_news_target(df)\n",
    "    y_dc = domain_target(df)\n",
    "\n",
    "    # Initializing the shuffle class instance\n",
    "    shuffle = Shuffle()\n",
    "    encoder_inputs = shuffle.shuffle(encoder_inputs)\n",
    "    y_cc = shuffle.shuffle(y_cc)\n",
    "    y_dc = shuffle.shuffle(y_dc)  # Not used\n",
    "\n",
    "    word2idx, idx2word, tokenizer = create_vocab(encoder_inputs, vocab_size)\n",
    "    vocab_len = len(word2idx)\n",
    "\n",
    "    encoder_inputs = pad_tokenize_data(\n",
    "        encoder_inputs, max_sentence_length, tokenizer)\n",
    "\n",
    "    encoder_inputs, y_cc, y_dc = all_data_generator(\n",
    "        encoder_inputs, y_cc, y_dc, max_sentence_length)\n",
    "\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(\n",
    "        encoder_inputs, y_cc, test_size=0.4, random_state=42)\n",
    "\n",
    "    return [[train_X, test_X, train_Y, test_Y], [word2idx, idx2word, tokenizer], vocab_len]\n",
    "\n",
    "\n",
    "def prepare_testing_data(df, max_sentence_length):\n",
    "    encoder_inputs = encoder_data(df)\n",
    "    y_cc = fake_news_target(df)\n",
    "    y_dc = domain_target(df)\n",
    "\n",
    "    encoder_inputs = pad_tokenize_data(\n",
    "        encoder_inputs, max_sentence_length, tokenizer)\n",
    "\n",
    "    encoder_inputs, y_cc, y_dc = all_data_generator(\n",
    "        encoder_inputs, y_cc, y_dc, max_sentence_length)\n",
    "\n",
    "    return [encoder_inputs, y_cc, y_dc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 50\n",
    "embedding_dim = 100\n",
    "latent_dim = 64\n",
    "vocab_size = 500\n",
    "\n",
    "[train_X, test_X, train_Y, test_Y], \\\n",
    "    [word2idx, idx2word, tokenizer], vocab_len = prepare_training_data(\n",
    "        d1, vocab_size, max_sentence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_inputs (InputLayer)  [(None, 50)]             0         \n",
      "                                                                 \n",
      " encoder_embedding (Embeddin  (None, 50, 100)          50200     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               [(None, 64),              42240     \n",
      "                              (None, 64),                        \n",
      "                              (None, 64)]                        \n",
      "                                                                 \n",
      " non_linear (Dense)          (None, 128)               8320      \n",
      "                                                                 \n",
      " softmax (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,018\n",
      "Trainable params: 101,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cc_model = classification_model(\n",
    "    max_sentence_length, embedding_dim, latent_dim, vocab_len)\n",
    "cc_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "95/95 [==============================] - 12s 99ms/step - loss: 0.3099 - accuracy: 0.8657 - val_loss: 0.2050 - val_accuracy: 0.9258 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "95/95 [==============================] - 8s 88ms/step - loss: 0.1894 - accuracy: 0.9331 - val_loss: 0.1839 - val_accuracy: 0.9365 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "95/95 [==============================] - 9s 91ms/step - loss: 0.1742 - accuracy: 0.9394 - val_loss: 0.1869 - val_accuracy: 0.9313 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "95/95 [==============================] - 9s 92ms/step - loss: 0.1652 - accuracy: 0.9412 - val_loss: 0.1845 - val_accuracy: 0.9358 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "95/95 [==============================] - 9s 90ms/step - loss: 0.1551 - accuracy: 0.9434 - val_loss: 0.1747 - val_accuracy: 0.9395 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "95/95 [==============================] - 9s 92ms/step - loss: 0.1486 - accuracy: 0.9470 - val_loss: 0.2077 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "95/95 [==============================] - 9s 91ms/step - loss: 0.1391 - accuracy: 0.9510 - val_loss: 0.1858 - val_accuracy: 0.9291 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "95/95 [==============================] - 9s 92ms/step - loss: 0.1330 - accuracy: 0.9527 - val_loss: 0.1797 - val_accuracy: 0.9332 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "95/95 [==============================] - 9s 91ms/step - loss: 0.1299 - accuracy: 0.9531 - val_loss: 0.1832 - val_accuracy: 0.9439 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "95/95 [==============================] - 9s 92ms/step - loss: 0.1230 - accuracy: 0.9545 - val_loss: 0.1686 - val_accuracy: 0.9395 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "95/95 [==============================] - 9s 91ms/step - loss: 0.1184 - accuracy: 0.9561 - val_loss: 0.1822 - val_accuracy: 0.9339 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "95/95 [==============================] - 9s 91ms/step - loss: 0.1143 - accuracy: 0.9590 - val_loss: 0.1878 - val_accuracy: 0.9402 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "95/95 [==============================] - 9s 94ms/step - loss: 0.1098 - accuracy: 0.9601 - val_loss: 0.1932 - val_accuracy: 0.9380 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "95/95 [==============================] - 9s 92ms/step - loss: 0.1050 - accuracy: 0.9621 - val_loss: 0.1846 - val_accuracy: 0.9402 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "95/95 [==============================] - 9s 92ms/step - loss: 0.1013 - accuracy: 0.9634 - val_loss: 0.1814 - val_accuracy: 0.9417 - lr: 0.0010\n",
      "Epoch 00015: early stopping\n"
     ]
    }
   ],
   "source": [
    "cc_model.compile(optimizer=\"rmsprop\",\n",
    "                 loss='binary_crossentropy', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1)\n",
    "mcp_save = ModelCheckpoint(\n",
    "    '.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')\n",
    "history = cc_model.fit(train_X,\n",
    "                       train_Y,\n",
    "                       batch_size=256,\n",
    "                       validation_split=0.1,\n",
    "                       callbacks=[es, mcp_save, reduce_lr_loss],\n",
    "                       epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction acuracy on same domain is 0.947\n",
      "Precision on same domain is 0.953\n",
      "Recall on same domain is 0.936\n",
      "F1 on same domain is 0.944\n",
      "           pred:Fake  pred:Real\n",
      "true:Fake       8061        555\n",
      "true:Real        396       8948\n",
      "\n",
      "\n",
      "Prediction acuracy on different domain is 0.459\n",
      "Precision on different domain is 0.632\n",
      "Recall on different domain is 0.09\n",
      "F1 on different domain is 0.157\n",
      "           pred:Fake  pred:Real\n",
      "true:Fake        515       5237\n",
      "true:Real        300       4188\n",
      "\n",
      "\n",
      "Prediction acuracy on different domain is 0.705\n",
      "Precision on different domain is 0.884\n",
      "Recall on different domain is 0.425\n",
      "F1 on different domain is 0.574\n",
      "           pred:Fake  pred:Real\n",
      "true:Fake        795       1077\n",
      "true:Real        104       2033\n",
      "\n",
      "\n",
      "Prediction acuracy on different domain is 0.465\n",
      "Precision on different domain is 0.656\n",
      "Recall on different domain is 0.091\n",
      "F1 on different domain is 0.16\n",
      "           pred:Fake  pred:Real\n",
      "true:Fake        411       4096\n",
      "true:Real        216       3338\n"
     ]
    }
   ],
   "source": [
    "y_pred = cc_model.predict(test_X)\n",
    "y_pred = np.array([np.argmax(x) for x in y_pred])\n",
    "y_true = np.array([np.argmax(x) for x in test_Y])\n",
    "\n",
    "print(f\"Prediction acuracy on same domain is {round(accuracy_score(y_true, y_pred), 3)}\")\n",
    "print(f\"Precision on same domain is {round(precision_score(y_true, y_pred), 3)}\")\n",
    "print(f\"Recall on same domain is {round(recall_score(y_true, y_pred), 3)}\")\n",
    "print(f\"F1 on same domain is {round(f1_score(y_true, y_pred), 3)}\")\n",
    "\n",
    "cmtx = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred, labels=[1, 0]),\n",
    "    index=['true:Fake', 'true:Real'],\n",
    "    columns=['pred:Fake', 'pred:Real']\n",
    ")\n",
    "print(cmtx)\n",
    "\n",
    "\n",
    "for df in [d2, d3, d4]:\n",
    "    df_encoder_inputs, df_y_cc, df_y_dc = prepare_testing_data(\n",
    "        df, max_sentence_length)\n",
    "\n",
    "    y_pred = cc_model.predict(df_encoder_inputs)\n",
    "    y_pred = np.array([np.argmax(x) for x in y_pred])\n",
    "    y_true = np.array([np.argmax(x) for x in df_y_cc])\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(f\"Prediction acuracy on different domain is {round(accuracy_score(y_true, y_pred), 3)}\")\n",
    "    print(f\"Precision on different domain is {round(precision_score(y_true, y_pred), 3)}\")\n",
    "    print(f\"Recall on different domain is {round(recall_score(y_true, y_pred), 3)}\")\n",
    "    print(f\"F1 on different domain is {round(f1_score(y_true, y_pred), 3)}\")\n",
    "    cmtx = pd.DataFrame(\n",
    "        confusion_matrix(y_true, y_pred, labels=[1, 0]),\n",
    "        index=['true:Fake', 'true:Real'],\n",
    "        columns=['pred:Fake', 'pred:Real']\n",
    "    )\n",
    "    print(cmtx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 50\n",
    "embedding_dim = 100\n",
    "latent_dim = 64\n",
    "vocab_size = 500\n",
    "\n",
    "[train_X, test_X, train_Y, test_Y], \\\n",
    "    [word2idx, idx2word, tokenizer], vocab_len = prepare_training_data(\n",
    "        pd.concat([d1, d2, d3, d4]), vocab_size, max_sentence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "142/142 [==============================] - 24s 147ms/step - loss: 0.5140 - accuracy: 0.7343 - val_loss: 0.3498 - val_accuracy: 0.8284 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "142/142 [==============================] - 21s 145ms/step - loss: 0.3459 - accuracy: 0.8291 - val_loss: 0.3289 - val_accuracy: 0.8344 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "142/142 [==============================] - 21s 148ms/step - loss: 0.3263 - accuracy: 0.8413 - val_loss: 0.3322 - val_accuracy: 0.8311 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "142/142 [==============================] - 21s 148ms/step - loss: 0.3136 - accuracy: 0.8465 - val_loss: 0.3389 - val_accuracy: 0.8354 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "142/142 [==============================] - 21s 147ms/step - loss: 0.3048 - accuracy: 0.8521 - val_loss: 0.3314 - val_accuracy: 0.8329 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "142/142 [==============================] - 21s 148ms/step - loss: 0.2973 - accuracy: 0.8576 - val_loss: 0.3309 - val_accuracy: 0.8354 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "142/142 [==============================] - 21s 149ms/step - loss: 0.2917 - accuracy: 0.8614 - val_loss: 0.3416 - val_accuracy: 0.8277 - lr: 0.0010\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cc_model.compile(optimizer=\"rmsprop\",\n",
    "                 loss='binary_crossentropy', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1)\n",
    "mcp_save = ModelCheckpoint(\n",
    "    '.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')\n",
    "history = cc_model.fit(train_X,\n",
    "                       train_Y,\n",
    "                       batch_size=256,\n",
    "                       validation_split=0.1,\n",
    "                       callbacks=[es, mcp_save, reduce_lr_loss],\n",
    "                       epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction acuracy on all domain is 0.831\n",
      "Precision on all domain is 0.859\n",
      "Recall on all domain is 0.79\n",
      "F1 on all domain is 0.823\n",
      "           pred:Fake  pred:Real\n",
      "true:Fake      10584       2817\n",
      "true:Real       1738      11745\n",
      "\n",
      "\n",
      "Prediction acuracy on different domain is 0.939\n",
      "Precision on different domain is 0.961\n",
      "Recall on different domain is 0.91\n",
      "F1 on different domain is 0.935\n",
      "           pred:Fake  pred:Real\n",
      "true:Fake      19490       1927\n",
      "true:Real        797      22684\n",
      "\n",
      "\n",
      "Prediction acuracy on different domain is 0.621\n",
      "Precision on different domain is 0.676\n",
      "Recall on different domain is 0.626\n",
      "F1 on different domain is 0.65\n",
      "           pred:Fake  pred:Real\n",
      "true:Fake       3599       2153\n",
      "true:Real       1726       2762\n",
      "\n",
      "\n",
      "Prediction acuracy on different domain is 0.807\n",
      "Precision on different domain is 0.972\n",
      "Recall on different domain is 0.604\n",
      "F1 on different domain is 0.745\n",
      "           pred:Fake  pred:Real\n",
      "true:Fake       1130        742\n",
      "true:Real         33       2104\n",
      "\n",
      "\n",
      "Prediction acuracy on different domain is 0.646\n",
      "Precision on different domain is 0.697\n",
      "Recall on different domain is 0.648\n",
      "F1 on different domain is 0.672\n",
      "           pred:Fake  pred:Real\n",
      "true:Fake       2922       1585\n",
      "true:Real       1268       2286\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = cc_model.predict(test_X)\n",
    "y_pred = np.array([np.argmax(x) for x in y_pred])\n",
    "y_true = np.array([np.argmax(x) for x in test_Y])\n",
    "\n",
    "print(f\"Prediction acuracy on all domain is {round(accuracy_score(y_true, y_pred), 3)}\")\n",
    "print(f\"Precision on all domain is {round(precision_score(y_true, y_pred), 3)}\")\n",
    "print(f\"Recall on all domain is {round(recall_score(y_true, y_pred), 3)}\")\n",
    "print(f\"F1 on all domain is {round(f1_score(y_true, y_pred), 3)}\")\n",
    "\n",
    "cmtx = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred, labels=[1, 0]),\n",
    "    index=['true:Fake', 'true:Real'],\n",
    "    columns=['pred:Fake', 'pred:Real']\n",
    ")\n",
    "print(cmtx)\n",
    "\n",
    "\n",
    "for df in [d1, d2, d3, d4]:\n",
    "    df_encoder_inputs, df_y_cc, df_y_dc = prepare_testing_data(\n",
    "        df, max_sentence_length)\n",
    "\n",
    "    y_pred = cc_model.predict(df_encoder_inputs)\n",
    "    y_pred = np.array([np.argmax(x) for x in y_pred])\n",
    "    y_true = np.array([np.argmax(x) for x in df_y_cc])\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(f\"Prediction acuracy on different domain is {round(accuracy_score(y_true, y_pred), 3)}\")\n",
    "    print(f\"Precision on different domain is {round(precision_score(y_true, y_pred), 3)}\")\n",
    "    print(f\"Recall on different domain is {round(recall_score(y_true, y_pred), 3)}\")\n",
    "    print(f\"F1 on different domain is {round(f1_score(y_true, y_pred), 3)}\")\n",
    "    cmtx = pd.DataFrame(\n",
    "        confusion_matrix(y_true, y_pred, labels=[1, 0]),\n",
    "        index=['true:Fake', 'true:Real'],\n",
    "        columns=['pred:Fake', 'pred:Real']\n",
    "    )\n",
    "    print(cmtx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(df, vocab_size, max_sentence_length):\n",
    "    encoder_inputs = encoder_data(df)\n",
    "    y_cc = fake_news_target(df)\n",
    "    y_dc = domain_target(df)\n",
    "\n",
    "    # Initializing the shuffle class instance\n",
    "    shuffle = Shuffle()\n",
    "    encoder_inputs = shuffle.shuffle(encoder_inputs)\n",
    "    y_cc = shuffle.shuffle(y_cc)\n",
    "    y_dc = shuffle.shuffle(y_dc)\n",
    "\n",
    "    word2idx, idx2word, tokenizer = create_vocab(encoder_inputs, vocab_size)\n",
    "    vocab_len = len(word2idx)\n",
    "\n",
    "    encoder_inputs = pad_tokenize_data(\n",
    "        encoder_inputs, max_sentence_length, tokenizer)\n",
    "\n",
    "    encoder_inputs, y_cc, y_dc = all_data_generator(\n",
    "        encoder_inputs, y_cc, y_dc, max_sentence_length)\n",
    "\n",
    "    train_X, test_X, train_C_Y, test_C_Y, train_D_Y, test_D_Y = train_test_split(\n",
    "        encoder_inputs, y_cc, y_dc, test_size=0.4, random_state=42)\n",
    "\n",
    "    return [[train_X, test_X, train_C_Y, test_C_Y, train_D_Y, test_D_Y], [word2idx, idx2word, tokenizer], vocab_len]\n",
    "\n",
    "def prepare_testing_data(df, max_sentence_length):\n",
    "    encoder_inputs = encoder_data(df)\n",
    "    y_cc = fake_news_target(df)\n",
    "    y_dc = domain_target(df)\n",
    "\n",
    "    encoder_inputs = pad_tokenize_data(\n",
    "        encoder_inputs, max_sentence_length, tokenizer)\n",
    "\n",
    "    encoder_inputs, y_cc, y_dc = all_data_generator(\n",
    "        encoder_inputs, y_cc, y_dc, max_sentence_length)\n",
    "\n",
    "    return [encoder_inputs, y_cc, y_dc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 100\n",
    "embedding_dim = 100\n",
    "latent_dim = 64\n",
    "vocab_size = 500\n",
    "\n",
    "[train_X, test_X, train_C_Y, test_C_Y, train_D_Y, test_D_Y], \\\n",
    "    [word2idx, idx2word, tokenizer], vocab_len = prepare_training_data(\n",
    "        pd.concat([d1, d2, d3, d4]), vocab_size, max_sentence_length)\n",
    "\n",
    "cc_model = classification_model(\n",
    "    max_sentence_length, embedding_dim, latent_dim, vocab_len)\n",
    "cc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " encoder_embedding (Embedding)  (None, 100, 100)     50200       ['encoder_inputs[0][0]']         \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  [(None, 64),         42240       ['encoder_embedding[0][0]']      \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " nonlinear_cc (Dense)           (None, 128)          8320        ['lstm_5[0][0]']                 \n",
      "                                                                                                  \n",
      " nonlinear_dc (Dense)           (None, 128)          8320        ['lstm_5[0][0]']                 \n",
      "                                                                                                  \n",
      " softmax_cc (Dense)             (None, 2)            258         ['nonlinear_cc[0][0]']           \n",
      "                                                                                                  \n",
      " softmax_dc (Dense)             (None, 4)            516         ['nonlinear_dc[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,854\n",
      "Trainable params: 109,854\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dc_model = domain_independent_model(\n",
    "    max_sentence_length, embedding_dim, latent_dim, vocab_len, domain_count=4)\n",
    "dc_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "142/142 [==============================] - 37s 237ms/step - loss: -2.2183 - softmax_cc_loss: 0.6972 - softmax_dc_loss: 13.5317 - softmax_cc_accuracy: 0.5163 - softmax_dc_accuracy: 0.0617 - val_loss: -3.9013 - val_softmax_cc_loss: 0.6697 - val_softmax_dc_loss: 21.8504 - val_softmax_cc_accuracy: 0.5986 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "142/142 [==============================] - 31s 216ms/step - loss: -5.4494 - softmax_cc_loss: 0.6125 - softmax_dc_loss: 29.3908 - softmax_cc_accuracy: 0.6796 - softmax_dc_accuracy: 0.0608 - val_loss: -6.9182 - val_softmax_cc_loss: 0.7273 - val_softmax_dc_loss: 37.1366 - val_softmax_cc_accuracy: 0.6561 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "142/142 [==============================] - 30s 213ms/step - loss: -8.5711 - softmax_cc_loss: 0.5160 - softmax_dc_loss: 44.6616 - softmax_cc_accuracy: 0.7492 - softmax_dc_accuracy: 0.0608 - val_loss: -10.1731 - val_softmax_cc_loss: 0.4424 - val_softmax_dc_loss: 52.4138 - val_softmax_cc_accuracy: 0.7922 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "142/142 [==============================] - 30s 213ms/step - loss: -11.6662 - softmax_cc_loss: 0.4510 - softmax_dc_loss: 59.9095 - softmax_cc_accuracy: 0.7884 - softmax_dc_accuracy: 0.0608 - val_loss: -12.9939 - val_softmax_cc_loss: 0.7788 - val_softmax_dc_loss: 67.6955 - val_softmax_cc_accuracy: 0.6764 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "142/142 [==============================] - 31s 216ms/step - loss: -14.7533 - softmax_cc_loss: 0.4002 - softmax_dc_loss: 75.1674 - softmax_cc_accuracy: 0.8090 - softmax_dc_accuracy: 0.0608 - val_loss: -16.3079 - val_softmax_cc_loss: 0.4072 - val_softmax_dc_loss: 82.9646 - val_softmax_cc_accuracy: 0.8019 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "142/142 [==============================] - 31s 216ms/step - loss: -17.8341 - softmax_cc_loss: 0.3606 - softmax_dc_loss: 90.4325 - softmax_cc_accuracy: 0.8243 - softmax_dc_accuracy: 0.0608 - val_loss: -19.3676 - val_softmax_cc_loss: 0.4044 - val_softmax_dc_loss: 98.2532 - val_softmax_cc_accuracy: 0.8076 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "142/142 [==============================] - 31s 218ms/step - loss: -20.8938 - softmax_cc_loss: 0.3437 - softmax_dc_loss: 105.6722 - softmax_cc_accuracy: 0.8313 - softmax_dc_accuracy: 0.0608 - val_loss: -22.4608 - val_softmax_cc_loss: 0.3493 - val_softmax_dc_loss: 113.5270 - val_softmax_cc_accuracy: 0.8217 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "142/142 [==============================] - 31s 216ms/step - loss: -23.9580 - softmax_cc_loss: 0.3283 - softmax_dc_loss: 120.9388 - softmax_cc_accuracy: 0.8370 - softmax_dc_accuracy: 0.0608 - val_loss: -25.5033 - val_softmax_cc_loss: 0.3689 - val_softmax_dc_loss: 128.8080 - val_softmax_cc_accuracy: 0.8197 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "142/142 [==============================] - 31s 215ms/step - loss: -27.0169 - softmax_cc_loss: 0.3160 - softmax_dc_loss: 136.1905 - softmax_cc_accuracy: 0.8455 - softmax_dc_accuracy: 0.0608 - val_loss: -28.5777 - val_softmax_cc_loss: 0.3398 - val_softmax_dc_loss: 144.0779 - val_softmax_cc_accuracy: 0.8262 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "142/142 [==============================] - 31s 216ms/step - loss: -30.0747 - softmax_cc_loss: 0.3068 - softmax_dc_loss: 151.4469 - softmax_cc_accuracy: 0.8494 - softmax_dc_accuracy: 0.0608 - val_loss: -31.6401 - val_softmax_cc_loss: 0.3299 - val_softmax_dc_loss: 159.3550 - val_softmax_cc_accuracy: 0.8356 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "142/142 [==============================] - 31s 218ms/step - loss: -33.1337 - softmax_cc_loss: 0.2977 - softmax_dc_loss: 166.7104 - softmax_cc_accuracy: 0.8568 - softmax_dc_accuracy: 0.0608 - val_loss: -34.6873 - val_softmax_cc_loss: 0.3430 - val_softmax_dc_loss: 174.6372 - val_softmax_cc_accuracy: 0.8302 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "142/142 [==============================] - 31s 216ms/step - loss: -36.1897 - softmax_cc_loss: 0.2922 - softmax_dc_loss: 181.9710 - softmax_cc_accuracy: 0.8586 - softmax_dc_accuracy: 0.0608 - val_loss: -37.7509 - val_softmax_cc_loss: 0.3312 - val_softmax_dc_loss: 189.9135 - val_softmax_cc_accuracy: 0.8408 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "142/142 [==============================] - 31s 218ms/step - loss: -39.2422 - softmax_cc_loss: 0.2858 - softmax_dc_loss: 197.2113 - softmax_cc_accuracy: 0.8635 - softmax_dc_accuracy: 0.0608 - val_loss: -40.8033 - val_softmax_cc_loss: 0.3354 - val_softmax_dc_loss: 205.1904 - val_softmax_cc_accuracy: 0.8351 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "142/142 [==============================] - 31s 218ms/step - loss: -42.2990 - softmax_cc_loss: 0.2810 - softmax_dc_loss: 212.4785 - softmax_cc_accuracy: 0.8643 - softmax_dc_accuracy: 0.0608 - val_loss: -43.8656 - val_softmax_cc_loss: 0.3261 - val_softmax_dc_loss: 220.4691 - val_softmax_cc_accuracy: 0.8450 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "142/142 [==============================] - 31s 216ms/step - loss: -45.3558 - softmax_cc_loss: 0.2746 - softmax_dc_loss: 227.7404 - softmax_cc_accuracy: 0.8714 - softmax_dc_accuracy: 0.0608 - val_loss: -46.9254 - val_softmax_cc_loss: 0.3200 - val_softmax_dc_loss: 235.7473 - val_softmax_cc_accuracy: 0.8475 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "142/142 [==============================] - 31s 217ms/step - loss: -48.4096 - softmax_cc_loss: 0.2698 - softmax_dc_loss: 242.9922 - softmax_cc_accuracy: 0.8723 - softmax_dc_accuracy: 0.0608 - val_loss: -49.9816 - val_softmax_cc_loss: 0.3191 - val_softmax_dc_loss: 251.0249 - val_softmax_cc_accuracy: 0.8495 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "142/142 [==============================] - 31s 221ms/step - loss: -51.4683 - softmax_cc_loss: 0.2646 - softmax_dc_loss: 258.2675 - softmax_cc_accuracy: 0.8741 - softmax_dc_accuracy: 0.0608 - val_loss: -53.0405 - val_softmax_cc_loss: 0.3134 - val_softmax_dc_loss: 266.2995 - val_softmax_cc_accuracy: 0.8473 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "142/142 [==============================] - 31s 218ms/step - loss: -54.5193 - softmax_cc_loss: 0.2604 - softmax_dc_loss: 273.5078 - softmax_cc_accuracy: 0.8780 - softmax_dc_accuracy: 0.0608 - val_loss: -56.0848 - val_softmax_cc_loss: 0.3287 - val_softmax_dc_loss: 281.5746 - val_softmax_cc_accuracy: 0.8455 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "142/142 [==============================] - 30s 214ms/step - loss: -57.5726 - softmax_cc_loss: 0.2572 - softmax_dc_loss: 288.7634 - softmax_cc_accuracy: 0.8778 - softmax_dc_accuracy: 0.0608 - val_loss: -59.1437 - val_softmax_cc_loss: 0.3245 - val_softmax_dc_loss: 296.8542 - val_softmax_cc_accuracy: 0.8443 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "142/142 [==============================] - 31s 217ms/step - loss: -60.6273 - softmax_cc_loss: 0.2524 - softmax_dc_loss: 304.0197 - softmax_cc_accuracy: 0.8813 - softmax_dc_accuracy: 0.0608 - val_loss: -62.1810 - val_softmax_cc_loss: 0.3492 - val_softmax_dc_loss: 312.1270 - val_softmax_cc_accuracy: 0.8450 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "142/142 [==============================] - 31s 218ms/step - loss: -63.6818 - softmax_cc_loss: 0.2491 - softmax_dc_loss: 319.2812 - softmax_cc_accuracy: 0.8834 - softmax_dc_accuracy: 0.0608 - val_loss: -65.2495 - val_softmax_cc_loss: 0.3334 - val_softmax_dc_loss: 327.4145 - val_softmax_cc_accuracy: 0.8430 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "142/142 [==============================] - 30s 214ms/step - loss: -66.7327 - softmax_cc_loss: 0.2450 - softmax_dc_loss: 334.5209 - softmax_cc_accuracy: 0.8848 - softmax_dc_accuracy: 0.0608 - val_loss: -68.3073 - val_softmax_cc_loss: 0.3289 - val_softmax_dc_loss: 342.6875 - val_softmax_cc_accuracy: 0.8480 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 23/30\n",
      "142/142 [==============================] - 31s 215ms/step - loss: -69.7888 - softmax_cc_loss: 0.2417 - softmax_dc_loss: 349.7900 - softmax_cc_accuracy: 0.8861 - softmax_dc_accuracy: 0.0608 - val_loss: -71.3677 - val_softmax_cc_loss: 0.3219 - val_softmax_dc_loss: 357.9652 - val_softmax_cc_accuracy: 0.8485 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 24/30\n",
      "142/142 [==============================] - 31s 215ms/step - loss: -72.8444 - softmax_cc_loss: 0.2374 - softmax_dc_loss: 365.0527 - softmax_cc_accuracy: 0.8890 - softmax_dc_accuracy: 0.0608 - val_loss: -74.4146 - val_softmax_cc_loss: 0.3341 - val_softmax_dc_loss: 373.2426 - val_softmax_cc_accuracy: 0.8440 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 25/30\n",
      "142/142 [==============================] - 31s 217ms/step - loss: -75.8987 - softmax_cc_loss: 0.2335 - softmax_dc_loss: 380.3107 - softmax_cc_accuracy: 0.8910 - softmax_dc_accuracy: 0.0608 - val_loss: -77.4746 - val_softmax_cc_loss: 0.3280 - val_softmax_dc_loss: 388.5212 - val_softmax_cc_accuracy: 0.8470 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 26/30\n",
      "142/142 [==============================] - 31s 216ms/step - loss: -78.9487 - softmax_cc_loss: 0.2313 - softmax_dc_loss: 395.5533 - softmax_cc_accuracy: 0.8924 - softmax_dc_accuracy: 0.0608 - val_loss: -80.5219 - val_softmax_cc_loss: 0.3352 - val_softmax_dc_loss: 403.7824 - val_softmax_cc_accuracy: 0.8470 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 27/30\n",
      "142/142 [==============================] - 31s 218ms/step - loss: -82.0075 - softmax_cc_loss: 0.2255 - softmax_dc_loss: 410.8268 - softmax_cc_accuracy: 0.8969 - softmax_dc_accuracy: 0.0608 - val_loss: -83.5724 - val_softmax_cc_loss: 0.3459 - val_softmax_dc_loss: 419.0723 - val_softmax_cc_accuracy: 0.8455 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 28/30\n",
      "142/142 [==============================] - 30s 213ms/step - loss: -85.0590 - softmax_cc_loss: 0.2225 - softmax_dc_loss: 426.0737 - softmax_cc_accuracy: 0.8968 - softmax_dc_accuracy: 0.0608 - val_loss: -86.6258 - val_softmax_cc_loss: 0.3445 - val_softmax_dc_loss: 434.3350 - val_softmax_cc_accuracy: 0.8495 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 29/30\n",
      "142/142 [==============================] - 31s 219ms/step - loss: -88.1139 - softmax_cc_loss: 0.2186 - softmax_dc_loss: 441.3350 - softmax_cc_accuracy: 0.8995 - softmax_dc_accuracy: 0.0608 - val_loss: -89.6918 - val_softmax_cc_loss: 0.3307 - val_softmax_dc_loss: 449.6168 - val_softmax_cc_accuracy: 0.8470 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n",
      "Epoch 30/30\n",
      "142/142 [==============================] - 31s 218ms/step - loss: -91.1673 - softmax_cc_loss: 0.2156 - softmax_dc_loss: 456.5909 - softmax_cc_accuracy: 0.9010 - softmax_dc_accuracy: 0.0608 - val_loss: -92.7366 - val_softmax_cc_loss: 0.3471 - val_softmax_dc_loss: 464.8976 - val_softmax_cc_accuracy: 0.8497 - val_softmax_dc_accuracy: 0.0565 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "dc_model.compile(optimizer=\"rmsprop\", loss=[\n",
    "    'binary_crossentropy', 'binary_crossentropy'], loss_weights=[0.7, -0.2], metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=30, verbose=1)\n",
    "mcp_save = ModelCheckpoint(\n",
    "    '.md2_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')\n",
    "history = dc_model.fit(train_X,\n",
    "                       [train_C_Y, train_D_Y],\n",
    "                       batch_size=256,\n",
    "                       validation_split=0.1,\n",
    "                       callbacks=[es, mcp_save, reduce_lr_loss],\n",
    "                       epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction acuracy on all domains is 0.85\n",
      "Precision on all domains is 0.87\n",
      "Recall on all domains is 0.83\n",
      "F1 on all domains is 0.85\n",
      "           pred:Fake  pred:Real\n",
      "true:Fake      11156       2232\n",
      "true:Real       1692      11804\n",
      "\n",
      "\n",
      "Prediction acuracy on different domain is 0.966\n",
      "Precision on different domain is 0.966\n",
      "Recall on different domain is 0.963\n",
      "F1 on different domain is 0.964\n",
      "           pred:Fake  pred:Real\n",
      "true:Fake      20618        799\n",
      "true:Real        721      22760\n",
      "\n",
      "\n",
      "Prediction acuracy on different domain is 0.661\n",
      "Precision on different domain is 0.714\n",
      "Recall on different domain is 0.66\n",
      "F1 on different domain is 0.686\n",
      "           pred:Fake  pred:Real\n",
      "true:Fake       3796       1956\n",
      "true:Real       1517       2971\n",
      "\n",
      "\n",
      "Prediction acuracy on different domain is 0.868\n",
      "Precision on different domain is 0.962\n",
      "Recall on different domain is 0.746\n",
      "F1 on different domain is 0.841\n",
      "           pred:Fake  pred:Real\n",
      "true:Fake       1397        475\n",
      "true:Real         55       2082\n",
      "\n",
      "\n",
      "Prediction acuracy on different domain is 0.691\n",
      "Precision on different domain is 0.74\n",
      "Recall on different domain is 0.692\n",
      "F1 on different domain is 0.715\n",
      "           pred:Fake  pred:Real\n",
      "true:Fake       3117       1390\n",
      "true:Real       1098       2456\n"
     ]
    }
   ],
   "source": [
    "y_pred, _ = dc_model.predict(test_X)\n",
    "y_pred = np.array([np.argmax(x) for x in y_pred])\n",
    "y_true = np.array([np.argmax(x) for x in test_C_Y])\n",
    "\n",
    "print(f\"Prediction acuracy on all domains is {round(accuracy_score(y_true, y_pred),2)}\")\n",
    "print(f\"Precision on all domains is {round(precision_score(y_true, y_pred),2)}\")\n",
    "print(f\"Recall on all domains is {round(recall_score(y_true, y_pred),2)}\")\n",
    "print(f\"F1 on all domains is {round(f1_score(y_true, y_pred),2)}\")\n",
    "cmtx = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred, labels=[1, 0]),\n",
    "    index=['true:Fake', 'true:Real'],\n",
    "    columns=['pred:Fake', 'pred:Real']\n",
    ")\n",
    "print(cmtx)\n",
    "\n",
    "for df in [d1, d2, d3, d4]:\n",
    "    df_encoder_inputs, df_y_cc, df_y_dc = prepare_testing_data(\n",
    "        df, max_sentence_length)\n",
    "\n",
    "    y_pred, _ = dc_model.predict(df_encoder_inputs)\n",
    "    y_pred = np.array([np.argmax(x) for x in y_pred])\n",
    "    y_true = np.array([np.argmax(x) for x in df_y_cc])\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(f\"Prediction acuracy on different domain is {round(accuracy_score(y_true, y_pred), 3)}\")\n",
    "    print(f\"Precision on different domain is {round(precision_score(y_true, y_pred), 3)}\")\n",
    "    print(f\"Recall on different domain is {round(recall_score(y_true, y_pred), 3)}\")\n",
    "    print(f\"F1 on different domain is {round(f1_score(y_true, y_pred), 3)}\")\n",
    "    cmtx = pd.DataFrame(\n",
    "        confusion_matrix(y_true, y_pred, labels=[1, 0]),\n",
    "        index=['true:Fake', 'true:Real'],\n",
    "        columns=['pred:Fake', 'pred:Real']\n",
    "    )\n",
    "    print(cmtx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26ee63aec03465293304913ace14509afda36eda5e12b236c75d02cfb3700c17"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
