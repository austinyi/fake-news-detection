{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shengjie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Shengjie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_path = \"./data/processed_tsv/K1.tsv\"\n",
    "d2_path = \"./data/processed_tsv/K2.tsv\"\n",
    "\n",
    "d1 = pd.read_csv(d1_path, delimiter=\"\\t\")\n",
    "d2 = pd.read_csv(d2_path, delimiter=\"\\t\")\n",
    "d1['content'] = d1['content'].apply(lambda x: str(x))\n",
    "d2['content'] = d2['content'].apply(lambda x: str(x))\n",
    "d1['domain'] = 0\n",
    "d2['domain'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== d1 =====\n",
      "        label\n",
      "label       \n",
      "0      23481\n",
      "1      21417\n",
      "===== d2 =====\n",
      "        label\n",
      "label       \n",
      "0       4488\n",
      "1       5752\n"
     ]
    }
   ],
   "source": [
    "print('===== d1 =====\\n', d1.groupby(['label'])[['label']].count())\n",
    "print('===== d2 =====\\n', d2.groupby(['label'])[['label']].count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        self.porter = PorterStemmer()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "        text = str(text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"i'm\", \"i am\", text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"she's\", \"she is\", text)\n",
    "        text = re.sub(r\"it's\", \"it is\", text)\n",
    "        text = re.sub(r\"that's\", \"that is\", text)\n",
    "        text = re.sub(r\"what's\", \"that is\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text)\n",
    "        text = re.sub(r\"how's\", \"how is\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"can't\", \"cannot\", text)\n",
    "        text = re.sub(r\"n't\", \" not\", text)\n",
    "        text = re.sub(r\"n'\", \"ng\", text)\n",
    "        text = re.sub(r\"'bout\", \"about\", text)\n",
    "        text = re.sub(r\"'til\", \"until\", text)\n",
    "        text = re.sub(r\"[()\\\"_#/@;*%:{}<>`+=~|.!?,'$-\\[\\]]\", \"\", text)\n",
    "        text = re.sub(r\"[0-9]\", \"\", text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def lemmatizer(self, text):\n",
    "        sentence_words = nltk.word_tokenize(text)\n",
    "        ret_text = []\n",
    "        for word in sentence_words:\n",
    "            ret_text.append(self.wordnet_lemmatizer.lemmatize(word))\n",
    "\n",
    "        return \" \".join(ret_text)\n",
    "\n",
    "    def stemmer(self, text):\n",
    "        sentence_words = nltk.word_tokenize(text)\n",
    "        ret_text = []\n",
    "        for word in sentence_words:\n",
    "            ret_text.append(self.porter.stem(word))\n",
    "\n",
    "        return \" \".join(ret_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_data(df, lemmatize=False, stem=False):\n",
    "    preprocessor = Preprocess()\n",
    "    encoder_inputs = df['content'].apply(lambda x: preprocessor.clean_text(x))\n",
    "    if lemmatize:\n",
    "        encoder_inputs = df['content'].apply(\n",
    "            lambda x: preprocessor.lemmatizer(x))\n",
    "    if stem:\n",
    "        encoder_inputs = df['content'].apply(lambda x: preprocessor.stemmer(x))\n",
    "    encoder_inputs = np.array(encoder_inputs.values.tolist())\n",
    "    return encoder_inputs\n",
    "\n",
    "\n",
    "def fake_news_target(df):\n",
    "    return np.array(df['label'].apply(lambda x: int(x)).values.tolist())\n",
    "\n",
    "\n",
    "def domain_target(df):\n",
    "    return np.array(df['domain'].apply(lambda x: int(x)).values.tolist())\n",
    "\n",
    "\n",
    "class Shuffle:\n",
    "\n",
    "    def __init__(self, data_len):\n",
    "        self.idx = np.arange(data_len)\n",
    "        np.random.shuffle(self.idx)\n",
    "\n",
    "    def shuffle(self, data):\n",
    "        return np.array(data)[self.idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the word2idx, idx2word mapping using the Keras Tokenizer\n",
    "def create_vocab(text_lists, vocab_len):\n",
    "    tokenizer = Tokenizer(oov_token=\"<UNK>\")\n",
    "    tokenizer.fit_on_texts(text_lists)\n",
    "    # Due to ambiguity with regards to Keras Tokenizer num_words, below is a good enough fix,\n",
    "    # though it changes the tokenizer word_index outside of the class\n",
    "    num_words = vocab_len\n",
    "\n",
    "    sorted_by_word_count = sorted(\n",
    "        tokenizer.word_counts.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    tokenizer.word_index = {}\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    i = 0\n",
    "    for word, count in sorted_by_word_count:\n",
    "        if i == num_words:\n",
    "            break\n",
    "\n",
    "        # <= because tokenizer is 1 indexed\n",
    "        tokenizer.word_index[word] = i + 1\n",
    "        word2idx[word] = i+1\n",
    "        idx2word[i+1] = word\n",
    "        i += 1\n",
    "\n",
    "    tokenizer.word_index[tokenizer.oov_token] = num_words+1\n",
    "    word2idx[tokenizer.oov_token] = num_words+1\n",
    "    idx2word[num_words+1] = tokenizer.oov_token\n",
    "\n",
    "    return word2idx, idx2word, tokenizer\n",
    "\n",
    "\n",
    "def pad_tokenize_data(encoder_inputs, max_sentence_length, tokenizer):\n",
    "\n",
    "    t_encoder_inputs = tokenizer.texts_to_sequences(encoder_inputs)\n",
    "    t_encoder_inputs = pad_sequences(\n",
    "        t_encoder_inputs, maxlen=max_sentence_length, padding='post', truncating='post')\n",
    "\n",
    "    return t_encoder_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, Y_CC, Y_DC, max_sentence_length, word2idx, batch_size=128):\n",
    "\n",
    "    y_cc = to_categorical(Y_CC)\n",
    "    y_dc = to_categorical(Y_DC)\n",
    "    for idx in range(0, len(X), batch_size):\n",
    "        encoder_input = np.zeros((batch_size, max_sentence_length))\n",
    "        decoder_target = np.zeros(\n",
    "            (batch_size, max_sentence_length, len(word2idx)+1))\n",
    "        for j, input_seq in enumerate(X[idx:idx+batch_size]):\n",
    "            for i, word_idx in enumerate(input_seq):\n",
    "                encoder_input[j, i] = word_idx\n",
    "                decoder_target[j, i, word_idx] = 1\n",
    "\n",
    "        yield [encoder_input, [decoder_target, y_cc[idx:idx+batch_size], y_dc[idx:idx+batch_size]]]\n",
    "\n",
    "\n",
    "def all_data_generator(X, Y_CC, Y_DC, max_sentence_length):\n",
    "    encoder_input = np.zeros((len(X), max_sentence_length))\n",
    "    for j, input_seq in enumerate(X):\n",
    "        for i, word_idx in enumerate(input_seq):\n",
    "            encoder_input[j, i] = word_idx\n",
    "\n",
    "    y_cc = to_categorical(Y_CC)\n",
    "    y_dc = to_categorical(Y_DC)\n",
    "\n",
    "    return [encoder_input, y_cc, y_dc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model(max_encoder_len, embedding_dim, latent_dim, vocab_len, include_glove=False):\n",
    "    inputs = Input(shape=(max_encoder_len,), name=\"encoder_inputs\")\n",
    "\n",
    "    layer_embedding = Embedding(vocab_len+1, embedding_dim, trainable=True,\n",
    "                                input_length=max_encoder_len, mask_zero=True, name=\"encoder_embedding\")\n",
    "    layer_lstm = LSTM(latent_dim, return_state=True)\n",
    "    layer_nonlinear = Dense(128, activation=\"tanh\", name=\"non_linear\")\n",
    "    layer_softmax = Dense(2, activation=\"softmax\", name=\"softmax\")\n",
    "\n",
    "    embedded = layer_embedding(inputs)\n",
    "    encoded, _, _ = layer_lstm(embedded)\n",
    "    logits = layer_nonlinear(encoded)\n",
    "    outputs = layer_softmax(logits)\n",
    "\n",
    "    return Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Independent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_independent_model(max_encoder_len, embedding_dim, latent_dim, vocab_len, include_glove=False):\n",
    "    inputs = Input(shape=(max_encoder_len,), name=\"encoder_inputs\")\n",
    "\n",
    "    layer_embedding = Embedding(vocab_len+1, embedding_dim, trainable=True,\n",
    "                                input_length=max_encoder_len, mask_zero=True, name=\"encoder_embedding\")\n",
    "    layer_lstm = LSTM(latent_dim, return_state=True)\n",
    "    layer_nonlinear_cc = Dense(128, activation=\"tanh\", name=\"non_linear\")\n",
    "    layer_softmax_cc = Dense(2, activation=\"softmax\", name=\"softmax\")\n",
    "    layer_nonlinear_dc = Dense(128, activation=\"tanh\", name=\"non_linear\")\n",
    "    layer_softmax_dc = Dense(2, activation=\"softmax\", name=\"softmax\")\n",
    "\n",
    "    embedded = layer_embedding(inputs)\n",
    "    encoded, _, _ = layer_lstm(embedded)\n",
    "    logits_cc = layer_nonlinear_cc(encoded)\n",
    "    outputs_cc = layer_softmax_cc(logits_cc)\n",
    "    logits_dc = layer_nonlinear_dc(encoded)\n",
    "    outputs_dc = layer_softmax_dc(logits_dc)\n",
    "\n",
    "    return Model(inputs, [outputs_cc, outputs_dc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(df, vocab_size, max_sentence_length):\n",
    "    encoder_inputs = encoder_data(df)\n",
    "    y_cc = fake_news_target(df)\n",
    "    y_dc = domain_target(df)\n",
    "\n",
    "    # Initializing the shuffle class instance\n",
    "    shuffle = Shuffle(len(encoder_inputs))\n",
    "    encoder_inputs = shuffle.shuffle(encoder_inputs)\n",
    "    y_cc = shuffle.shuffle(y_cc)\n",
    "    y_dc = shuffle.shuffle(y_dc)  # Not used\n",
    "\n",
    "    word2idx, idx2word, tokenizer = create_vocab(encoder_inputs, vocab_size)\n",
    "    vocab_len = len(word2idx)\n",
    "\n",
    "    encoder_inputs = pad_tokenize_data(\n",
    "        encoder_inputs, max_sentence_length, tokenizer)\n",
    "\n",
    "    encoder_inputs, y_cc, y_dc = all_data_generator(\n",
    "        encoder_inputs, y_cc, y_dc, max_sentence_length)\n",
    "\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(\n",
    "        encoder_inputs, y_cc, test_size=0.1, random_state=42)\n",
    "\n",
    "    return [[train_X, test_X, train_Y, test_Y], [word2idx, idx2word, tokenizer], vocab_len]\n",
    "\n",
    "\n",
    "def prepare_testing_data(df, max_sentence_length):\n",
    "    encoder_inputs = encoder_data(df)\n",
    "    y_cc = fake_news_target(df)\n",
    "    y_dc = domain_target(df)\n",
    "\n",
    "    encoder_inputs = pad_tokenize_data(\n",
    "        encoder_inputs, max_sentence_length, tokenizer)\n",
    "\n",
    "    encoder_inputs, y_cc, y_dc = all_data_generator(\n",
    "        encoder_inputs, y_cc, y_dc, max_sentence_length)\n",
    "\n",
    "    return [encoder_inputs, y_cc, y_dc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 50\n",
    "embedding_dim = 100\n",
    "latent_dim = 64\n",
    "vocab_size = 500\n",
    "\n",
    "[train_X, test_X, train_Y, test_Y], \\\n",
    "    [word2idx, idx2word, tokenizer], vocab_len = prepare_training_data(\n",
    "        d1, vocab_size, max_sentence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_inputs (InputLayer)  [(None, 50)]             0         \n",
      "                                                                 \n",
      " encoder_embedding (Embeddin  (None, 50, 100)          50200     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " lstm (LSTM)                 [(None, 64),              42240     \n",
      "                              (None, 64),                        \n",
      "                              (None, 64)]                        \n",
      "                                                                 \n",
      " non_linear (Dense)          (None, 128)               8320      \n",
      "                                                                 \n",
      " softmax (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,018\n",
      "Trainable params: 101,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cc_model = classification_model(\n",
    "    max_sentence_length, embedding_dim, latent_dim, vocab_len, include_glove=False)\n",
    "cc_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "143/143 [==============================] - 11s 58ms/step - loss: 0.2761 - accuracy: 0.8871 - val_loss: 0.1847 - val_accuracy: 0.9362 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 0.1826 - accuracy: 0.9359 - val_loss: 0.2267 - val_accuracy: 0.9319 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "143/143 [==============================] - 7s 48ms/step - loss: 0.1697 - accuracy: 0.9388 - val_loss: 0.2434 - val_accuracy: 0.9277 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "143/143 [==============================] - 7s 48ms/step - loss: 0.1649 - accuracy: 0.9412 - val_loss: 0.1904 - val_accuracy: 0.9428 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "143/143 [==============================] - 7s 48ms/step - loss: 0.1531 - accuracy: 0.9442 - val_loss: 0.2004 - val_accuracy: 0.9337 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "143/143 [==============================] - 7s 51ms/step - loss: 0.1459 - accuracy: 0.9460 - val_loss: 0.1564 - val_accuracy: 0.9470 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "143/143 [==============================] - 8s 56ms/step - loss: 0.1388 - accuracy: 0.9484 - val_loss: 0.1504 - val_accuracy: 0.9483 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "143/143 [==============================] - 8s 57ms/step - loss: 0.1323 - accuracy: 0.9500 - val_loss: 0.2825 - val_accuracy: 0.8862 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 0.1293 - accuracy: 0.9501 - val_loss: 0.1497 - val_accuracy: 0.9483 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "143/143 [==============================] - 7s 51ms/step - loss: 0.1242 - accuracy: 0.9534 - val_loss: 0.1661 - val_accuracy: 0.9433 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 0.1190 - accuracy: 0.9548 - val_loss: 0.2077 - val_accuracy: 0.9243 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "143/143 [==============================] - 7s 48ms/step - loss: 0.1130 - accuracy: 0.9576 - val_loss: 0.1654 - val_accuracy: 0.9357 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "143/143 [==============================] - 7s 48ms/step - loss: 0.1079 - accuracy: 0.9591 - val_loss: 0.1479 - val_accuracy: 0.9461 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "143/143 [==============================] - 7s 47ms/step - loss: 0.1051 - accuracy: 0.9600 - val_loss: 0.2345 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "143/143 [==============================] - 7s 47ms/step - loss: 0.0987 - accuracy: 0.9636 - val_loss: 0.2361 - val_accuracy: 0.9181 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "143/143 [==============================] - 7s 48ms/step - loss: 0.0962 - accuracy: 0.9642 - val_loss: 0.1953 - val_accuracy: 0.9391 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "143/143 [==============================] - 7s 48ms/step - loss: 0.0925 - accuracy: 0.9654 - val_loss: 0.1471 - val_accuracy: 0.9508 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "143/143 [==============================] - 7s 47ms/step - loss: 0.0879 - accuracy: 0.9670 - val_loss: 0.1598 - val_accuracy: 0.9488 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "143/143 [==============================] - 7s 47ms/step - loss: 0.0842 - accuracy: 0.9682 - val_loss: 0.1528 - val_accuracy: 0.9480 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "143/143 [==============================] - 7s 48ms/step - loss: 0.0796 - accuracy: 0.9706 - val_loss: 0.1765 - val_accuracy: 0.9475 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "143/143 [==============================] - 8s 58ms/step - loss: 0.0756 - accuracy: 0.9734 - val_loss: 0.1863 - val_accuracy: 0.9470 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "143/143 [==============================] - 7s 50ms/step - loss: 0.0726 - accuracy: 0.9744 - val_loss: 0.1659 - val_accuracy: 0.9465 - lr: 0.0010\n",
      "Epoch 00022: early stopping\n"
     ]
    }
   ],
   "source": [
    "cc_model.compile(optimizer=\"rmsprop\",\n",
    "                 loss='binary_crossentropy', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1)\n",
    "mcp_save = ModelCheckpoint(\n",
    "    '.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')\n",
    "history = cc_model.fit(train_X,\n",
    "                       train_Y,\n",
    "                       batch_size=256,\n",
    "                       validation_split=0.1,\n",
    "                       callbacks=[es, mcp_save, reduce_lr_loss],\n",
    "                       epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction acuracy on same domain is 0.95\n",
      "Precision on same domain is 0.93\n",
      "Recall on same domain is 0.96\n",
      "F1 on same domain is 0.95\n",
      "\n",
      "\n",
      "           pred:Fake  pred:Real\n",
      "true:Fake       2104         88\n",
      "true:Real        156       2142\n",
      "Prediction acuracy on different domain is 0.46\n",
      "Precision on different domain is 0.62\n",
      "Recall on different domain is 0.1\n",
      "F1 on different domain is 0.18\n",
      "\n",
      "\n",
      "           pred:Fake  pred:Real\n",
      "true:Fake        600       5152\n",
      "true:Real        370       4118\n"
     ]
    }
   ],
   "source": [
    "y_pred = cc_model.predict(test_X)\n",
    "y_pred = np.array([np.argmax(x) for x in y_pred])\n",
    "y_true = np.array([np.argmax(x) for x in test_Y])\n",
    "\n",
    "print(\n",
    "    f\"Prediction acuracy on same domain is {round(accuracy_score(y_true,y_pred),2)}\")\n",
    "print(f\"Precision on same domain is {round(precision_score(y_true,y_pred),2)}\")\n",
    "print(f\"Recall on same domain is {round(recall_score(y_true,y_pred),2)}\")\n",
    "print(f\"F1 on same domain is {round(f1_score(y_true,y_pred),2)}\")\n",
    "\n",
    "cmtx = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred, labels=[1, 0]),\n",
    "    index=['true:Fake', 'true:Real'],\n",
    "    columns=['pred:Fake', 'pred:Real']\n",
    ")\n",
    "print(\"\\n\")\n",
    "print(cmtx)\n",
    "\n",
    "d2_encoder_inputs, d2_y_cc, d2_y_dc = prepare_testing_data(\n",
    "    d2, max_sentence_length)\n",
    "\n",
    "y_pred = cc_model.predict(d2_encoder_inputs)\n",
    "y_pred = np.array([np.argmax(x) for x in y_pred])\n",
    "y_true = np.array([np.argmax(x) for x in d2_y_cc])\n",
    "\n",
    "print(\n",
    "    f\"Prediction acuracy on different domain is {round(accuracy_score(y_true,y_pred),2)}\")\n",
    "print(\n",
    "    f\"Precision on different domain is {round(precision_score(y_true,y_pred),2)}\")\n",
    "print(f\"Recall on different domain is {round(recall_score(y_true,y_pred),2)}\")\n",
    "print(f\"F1 on different domain is {round(f1_score(y_true,y_pred),2)}\")\n",
    "cmtx = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred, labels=[1, 0]),\n",
    "    index=['true:Fake', 'true:Real'],\n",
    "    columns=['pred:Fake', 'pred:Real']\n",
    ")\n",
    "print(\"\\n\")\n",
    "print(cmtx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(df, vocab_size, max_sentence_length):\n",
    "    encoder_inputs = encoder_data(df)\n",
    "    y_cc = fake_news_target(df)\n",
    "    y_dc = domain_target(df)\n",
    "\n",
    "    # Initializing the shuffle class instance\n",
    "    shuffle = Shuffle(len(encoder_inputs))\n",
    "    encoder_inputs = shuffle.shuffle(encoder_inputs)\n",
    "    y_cc = shuffle.shuffle(y_cc)\n",
    "    y_dc = shuffle.shuffle(y_dc)\n",
    "\n",
    "    word2idx, idx2word, tokenizer = create_vocab(encoder_inputs, vocab_size)\n",
    "    vocab_len = len(word2idx)\n",
    "\n",
    "    encoder_inputs = pad_tokenize_data(\n",
    "        encoder_inputs, max_sentence_length, tokenizer)\n",
    "\n",
    "    encoder_inputs, y_cc, y_dc = all_data_generator(\n",
    "        encoder_inputs, y_cc, y_dc, max_sentence_length)\n",
    "\n",
    "    train_X, test_X, train_C_Y, test_C_Y, train_D_Y, test_D_Y = train_test_split(\n",
    "        encoder_inputs, y_cc, y_dc, test_size=0.1, random_state=42)\n",
    "\n",
    "    return [[train_X, test_X, train_C_Y, test_C_Y, train_D_Y, test_D_Y], [word2idx, idx2word, tokenizer], vocab_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.77 GiB for an array with shape (55138,) and data type <U37846",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9420/3731299936.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_C_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_C_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_D_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_D_Y\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     [word2idx, idx2word, tokenizer], vocab_len = prepare_training_data(\n\u001b[0m\u001b[0;32m      8\u001b[0m         pd.concat([d1, d2]), vocab_size, max_sentence_length)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9420/2995304543.py\u001b[0m in \u001b[0;36mprepare_training_data\u001b[1;34m(df, vocab_size, max_sentence_length)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Initializing the shuffle class instance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mShuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mencoder_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0my_cc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_cc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0my_dc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_dc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9420/510751321.py\u001b[0m in \u001b[0;36mshuffle\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 7.77 GiB for an array with shape (55138,) and data type <U37846"
     ]
    }
   ],
   "source": [
    "max_sentence_length = 100\n",
    "embedding_dim = 100\n",
    "latent_dim = 64\n",
    "vocab_size = 500\n",
    "\n",
    "[train_X, test_X, train_C_Y, test_C_Y, train_D_Y, test_D_Y], \\\n",
    "    [word2idx, idx2word, tokenizer], vocab_len = prepare_training_data(\n",
    "        pd.concat([d1, d2]), vocab_size, max_sentence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_inputs (InputLayer)  [(None, 100)]            0         \n",
      "                                                                 \n",
      " encoder_embedding (Embeddin  (None, 100, 100)         50200     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               [(None, 64),              42240     \n",
      "                              (None, 64),                        \n",
      "                              (None, 64)]                        \n",
      "                                                                 \n",
      " non_linear (Dense)          (None, 128)               8320      \n",
      "                                                                 \n",
      " softmax (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,018\n",
      "Trainable params: 101,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dc_model = domain_independent_model(\n",
    "    max_sentence_length, embedding_dim, latent_dim, vocab_len, include_glove=False)\n",
    "dc_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 817, in train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 439, in update_state\n        self.build(y_pred, y_true)\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 359, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(y_pred, self._get_metric_objects,\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 485, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 485, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 506, in _get_metric_object\n        y_t_rank = len(y_t.shape.as_list())\n\n    AttributeError: 'tuple' object has no attribute 'shape'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1060/2834432522.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m reduce_lr_loss = ReduceLROnPlateau(\n\u001b[0;32m      7\u001b[0m     monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')\n\u001b[1;32m----> 8\u001b[1;33m history = dc_model.fit(train_X,\n\u001b[0m\u001b[0;32m      9\u001b[0m                     \u001b[1;33m[\u001b[0m\u001b[0mtrain_C_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_D_Y\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 817, in train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 439, in update_state\n        self.build(y_pred, y_true)\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 359, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(y_pred, self._get_metric_objects,\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 485, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 485, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"c:\\Users\\Shengjie\\Projects\\fake-news-detection\\.venv\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 506, in _get_metric_object\n        y_t_rank = len(y_t.shape.as_list())\n\n    AttributeError: 'tuple' object has no attribute 'shape'\n"
     ]
    }
   ],
   "source": [
    "dc_model.compile(optimizer=\"rmsprop\", loss=[\n",
    "              'binary_crossentropy', 'binary_crossentropy'], loss_weights=[0.7, -0.2], metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=30, verbose=1)\n",
    "mcp_save = ModelCheckpoint(\n",
    "    '.md2_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')\n",
    "history = dc_model.fit(train_X,\n",
    "                    [train_C_Y, train_D_Y],\n",
    "                    batch_size=256,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[es, mcp_save, reduce_lr_loss],\n",
    "                    epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred,_ = dc_model.predict(test_X)\n",
    "y_pred = np.array([np.argmax(x) for x in y_pred])\n",
    "y_true = np.array([np.argmax(x) for x in test_C_Y])\n",
    "\n",
    "print(f\"Prediction acuracy on both domains is {round(accuracy_score(y_true,y_pred),2)}\")\n",
    "print(f\"Precision on both domains is {round(precision_score(y_true,y_pred),2)}\")\n",
    "print(f\"Recall on both domains is {round(recall_score(y_true,y_pred),2)}\")\n",
    "print(f\"F1 on both domains is {round(f1_score(y_true,y_pred),2)}\")\n",
    "cmtx = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred, labels=[1, 0]), \n",
    "    index=['true:Fake', 'true:Real'], \n",
    "    columns=['pred:Fake', 'pred:Real']\n",
    ")\n",
    "print(\"\\n\")\n",
    "print(cmtx)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26ee63aec03465293304913ace14509afda36eda5e12b236c75d02cfb3700c17"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
